@inproceedings{wav2lip,
author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413532},
doi = {10.1145/3394171.3413532},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {484–492},
numpages = {9},
keywords = {lip sync, talking face generation, video generation},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{makeitalk,
author = {Zhou, Yang and Han, Xintong and Shechtman, Eli and Echevarria, Jose and Kalogerakis, Evangelos and Li, Dingzeyu},
title = {MakeltTalk: Speaker-Aware Talking-Head Animation},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417774},
doi = {10.1145/3414685.3417774},
abstract = {We present a method that generates expressive talking-head videos from a single facial image with audio as the only input. In contrast to previous attempts to learn direct mappings from audio to raw pixels for creating talking faces, our method first disentangles the content and speaker information in the input audio signal. The audio content robustly controls the motion of lips and nearby facial regions, while the speaker information determines the specifics of facial expressions and the rest of the talking-head dynamics. Another key component of our method is the prediction of facial landmarks reflecting the speaker-aware dynamics. Based on this intermediate representation, our method works with many portrait images in a single unified framework, including artistic paintings, sketches, 2D cartoon characters, Japanese mangas, and stylized caricatures. In addition, our method generalizes well for faces and characters that were not observed during training. We present extensive quantitative and qualitative evaluation of our method, in addition to user studies, demonstrating generated talking-heads of significantly higher quality compared to prior state-of-the-art methods.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {221},
numpages = {15},
keywords = {facial animation, neural networks}
}

@INPROCEEDINGS{Text2Video,  
author={Zhang, Sibo and Yuan, Jiahong and Liao, Miao and Zhang, Liangjun},  
booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   
title={Text2video: Text-Driven Talking-Head Video Synthesis with Personalized Phoneme - Pose Dictionary},   
year={2022},  
volume={},  
number={},  
pages={2659-2663},  
doi={10.1109/ICASSP43922.2022.9747380}
}

@misc{Vall-E-X,
      title={Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling}, 
      author={Ziqiang Zhang and Long Zhou and Chengyi Wang and Sanyuan Chen and Yu Wu and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},
      year={2023},
      eprint={2303.03926},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{Fastspeech2,
  author={Chien, Chung-Ming and Lin, Jheng-Hao and Huang, Chien-yu and Hsu, Po-chun and Lee, Hung-yi},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech}, 
  year={2021},
  volume={},
  number={},
  pages={8588-8592},
  doi={10.1109/ICASSP39728.2021.9413880}}

@inproceedings{mildenhall2020nerf,
  title        = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  author       = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  year         = 2020,
  booktitle    = {ECCV}
}

@article{tang2022radnerf,
  title={Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition},
  author={Tang, Jiaxiang and Wang, Kaisiyuan and Zhou, Hang and Chen, Xiaokang and He, Dongliang and Hu, Tianshu and Liu, Jingtuo and Zeng, Gang and Wang, Jingdong},
  journal={arXiv preprint arXiv:2211.12368},
  year={2022}
}

  @InProceedings{li2023ernerf,
    author    = {Li, Jiahe and Zhang, Jiawei and Bai, Xiao and Zhou, Jun and Gu, Lin},
    title     = {Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {7568-7578}
}

@article{ye2023geneface,
  title={GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis},
  author={Ye, Zhenhui and Jiang, Ziyue and Ren, Yi and Liu, Jinglin and He, Jinzheng and Zhao, Zhou},
  journal={arXiv preprint arXiv:2301.13430},
  year={2023}
}

@article{TensoRF,
  title        = {TensoRF: Tensorial Radiance Fields},
  author       = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
  year         = 2022,
  journal      = {arXiv preprint arXiv:2203.09517}
}

@article{mueller2022instant,
  title        = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author       = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
  year         = 2022,
  month        = jan,
  journal      = {arXiv:2201.05989}
}

@article{EnCodec,
  title={High Fidelity Neural Audio Compression},
  author={Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},
  journal={arXiv preprint arXiv:2210.13438},
  year={2022}
}

@inproceedings{hjortnaes:2020,
    author = {Nils Hjortnaes and Niko Partanen and Michael Rießler and Francis M. Tyers},
    title = {Towards a Speech Recognizer for Komi, an Endangered and Low-Resource Uralic Language},
    booktitle = {Proceedings of the 6th International Workshop on Computational Linguistics of Uralic Languages},
    year = 2020
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}


@article{suwajanakorn2017synthesizing,
  title={Synthesizing obama: learning lip sync from audio},
  author={Suwajanakorn, Supasorn and Seitz, Steven M and Kemelmacher-Shlizerman, Ira},
  journal={ACM Transactions on Graphics (ToG)},
  volume={36},
  number={4},
  pages={1--13},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{guo2021ad,
  title        = {Ad-nerf: Audio driven neural radiance fields for talking head synthesis},
  author       = {Guo, Yudong and Chen, Keyu and Liang, Sen and Liu, Yong-Jin and Bao, Hujun and Zhang, Juyong},
  year         = 2021,
  booktitle    = {ICCV},
  pages        = {5784--5794}
}

@inproceedings{zhang2018perceptual,
  title        = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author       = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  year         = 2018,
  booktitle    = {CVPR}
}

@article{zhou2020makelttalk,
  title        = {Makelttalk: speaker-aware talking-head animation},
  author       = {Zhou, Yang and Han, Xintong and Shechtman, Eli and Echevarria, Jose and Kalogerakis, Evangelos and Li, Dingzeyu},
  year         = 2020,
  journal      = {ACM TOG},
  publisher    = {ACM New York, NY, USA},
  volume       = 39,
  number       = 6,
  pages        = {1--15}
}

@inproceedings{prajwal2020lip,
  title        = {A lip sync expert is all you need for speech to lip generation in the wild},
  author       = {Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  year         = 2020,
  booktitle    = {ACM MM},
  pages        = {484--492}
}

@article{mitsui2023uniflg,
  title={UniFLG: Unified Facial Landmark Generator from Text or Speech},
  author={Mitsui, Kentaro and Hono, Yukiya and Sawada, Kei},
  journal={arXiv preprint arXiv:2302.14337},
  year={2023}
}